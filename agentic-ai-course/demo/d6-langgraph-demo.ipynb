{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "introduction",
   "metadata": {},
   "source": [
    "# LangGraph Demo\n",
    "\n",
    "This notebook demonstrates **LangGraph**, the most sophisticated framework for building stateful, multi-agent applications with complex workflows using graph-based architectures.\n",
    "\n",
    "## Required Environment Variables\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY    # Your OpenAI API key (required)\n",
    "```\n",
    "\n",
    "Please refer to the [README](README.md) for instructions on setting up environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dyfmn5ox4n",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If running standalone without the project setup:\n",
    "# pip install langgraph openai python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List, Literal, TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# Import LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# For visualization\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3kcxut4ul7i",
   "metadata": {},
   "source": [
    "### LangGraph Core Components Loaded:\n",
    "- **StateGraph:** Graph-based workflow orchestrator\n",
    "- **State Management:** TypedDict-based state schemas\n",
    "- **Conditional Logic:** Dynamic routing capabilities\n",
    "- **LLM Integration:** ChatOpenAI for intelligent node processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "state-concept",
   "metadata": {},
   "source": [
    "## üìä State Management\n",
    "\n",
    "In LangGraph, **State** is the cornerstone that makes complex workflows possible. Unlike other frameworks where data flows through function calls, LangGraph maintains a **persistent, shared state** that evolves through the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "state-definition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our state schema using TypedDict\n",
    "class TicketState(TypedDict):\n",
    "    \"\"\"State schema for our customer support ticket routing system\"\"\"\n",
    "    # Ticket information\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "\n",
    "    # Processing state\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "\n",
    "    # Workflow tracking\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]  # List reducer\n",
    "    status: str\n",
    "\n",
    "    # Results\n",
    "    resolution: str\n",
    "\n",
    "# Example initial state\n",
    "sample_state = TicketState(\n",
    "    ticket_id=\"TKT-12345\",\n",
    "    title=\"Login Issues with Mobile App\",\n",
    "    description=\"Customer unable to log in using mobile app, web works fine\",\n",
    "    customer_tier=\"Premium\",\n",
    "    category=\"\",\n",
    "    priority=\"\",\n",
    "    assigned_agent=\"\",\n",
    "    processing_steps=[],\n",
    "    status=\"New\",\n",
    "    resolution=\"\"\n",
    ")\n",
    "\n",
    "print(sample_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3up574omlus",
   "metadata": {},
   "source": [
    "### State Schema Defined: TicketState\n",
    "\n",
    "**State Fields:**\n",
    "- üìù **Ticket Info:** ticket_id, title, description, customer_tier\n",
    "- ‚öôÔ∏è **Processing:** category, priority, assigned_agent\n",
    "- üîÑ **Workflow:** processing_steps (with list reducer), status\n",
    "- ‚úÖ **Results:** resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mtswda27mbh",
   "metadata": {},
   "source": [
    "## üìä Visualizing the AI Workflow as a Graph\n",
    "\n",
    "Before we demonstrate the workflow execution, let's visualize how LangGraph structures the workflow as a graph. This shows how the state will flow through each AI-powered decision node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dk9lcgleghv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM for AI-powered workflow\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# Node 1: Ticket Classifier with LLM\n",
    "def classify_ticket(state: TicketState) -> dict:\n",
    "    \"\"\"Classify ticket into category using LLM analysis\"\"\"\n",
    "\n",
    "    print(f\"Classifying ticket: {state['ticket_id']}\")\n",
    "\n",
    "    # Use LLM to classify the ticket\n",
    "    classification_prompt = f\"\"\"\n",
    "    Analyze this support ticket and classify it into one of these categories:\n",
    "    - Technical: Login issues, bugs, errors, system problems\n",
    "    - Billing: Payment, subscription, pricing issues\n",
    "    - General: All other inquiries\n",
    "\n",
    "    Ticket Title: {state['title']}\n",
    "    Ticket Description: {state['description']}\n",
    "\n",
    "    Respond with ONLY the category name (Technical, Billing, or General).\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a ticket classification expert.\"),\n",
    "        HumanMessage(content=classification_prompt)\n",
    "    ])\n",
    "\n",
    "    category = response.content.strip()\n",
    "\n",
    "    # Validate and default to General if unexpected response\n",
    "    if category not in [\"Technical\", \"Billing\", \"General\"]:\n",
    "        category = \"General\"\n",
    "\n",
    "    # Return state update (partial update, not full state)\n",
    "    return {\n",
    "        \"category\": category,\n",
    "        \"processing_steps\": [f\"Classified as {category} by AI analysis\"],\n",
    "        \"status\": \"Classified\"\n",
    "    }\n",
    "\n",
    "# Node 2: Priority Router with LLM\n",
    "def route_priority(state: TicketState) -> dict:\n",
    "    \"\"\"Determine ticket priority using LLM reasoning\"\"\"\n",
    "\n",
    "    print(f\"Routing priority for {state['category']} ticket from {state['customer_tier']} customer\")\n",
    "\n",
    "    # Use LLM to determine priority\n",
    "    priority_prompt = f\"\"\"\n",
    "    Determine the priority level for this support ticket based on:\n",
    "    - Category: {state['category']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "\n",
    "    Priority Rules:\n",
    "    - Premium customers with Technical issues should be High priority\n",
    "    - Premium customers with other issues should be Medium priority\n",
    "    - Standard customers with Technical issues should be Medium priority\n",
    "    - Standard customers with other issues should be Low priority\n",
    "    - Critical system outages should always be High priority\n",
    "\n",
    "    Respond with ONLY the priority level (High, Medium, or Low).\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support ticket priority expert.\"),\n",
    "        HumanMessage(content=priority_prompt)\n",
    "    ])\n",
    "\n",
    "    priority = response.content.strip()\n",
    "\n",
    "    # Validate and set default\n",
    "    if priority not in [\"High\", \"Medium\", \"Low\"]:\n",
    "        priority = \"Medium\"\n",
    "\n",
    "    step_msg = f\"Priority set to {priority} by AI assessment ({state['customer_tier']} customer, {state['category']} issue)\"\n",
    "\n",
    "    return {\n",
    "        \"priority\": priority,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Prioritized\"\n",
    "    }\n",
    "\n",
    "# Node 3: Agent Matcher with LLM\n",
    "def match_agent(state: TicketState) -> dict:\n",
    "    \"\"\"Assign appropriate agent using LLM matching\"\"\"\n",
    "\n",
    "    print(f\"Matching agent for {state['priority']} priority {state['category']} ticket\")\n",
    "\n",
    "    # Use LLM to match the best agent\n",
    "    agent_prompt = f\"\"\"\n",
    "    Match the best support agent for this ticket:\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Issue: {state['title']}\n",
    "\n",
    "    Available agents:\n",
    "    - Senior Tech Support: For high-priority technical issues\n",
    "    - Tech Support: For standard technical issues\n",
    "    - Billing Specialist: For all billing-related issues\n",
    "    - General Support: For general inquiries and low-priority issues\n",
    "\n",
    "    Respond with ONLY the agent role name.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an expert at matching support tickets to the right agents.\"),\n",
    "        HumanMessage(content=agent_prompt)\n",
    "    ])\n",
    "\n",
    "    agent = response.content.strip()\n",
    "\n",
    "    # Validate agent assignment\n",
    "    valid_agents = [\"Senior Tech Support\", \"Tech Support\", \"Billing Specialist\", \"General Support\"]\n",
    "    if agent not in valid_agents:\n",
    "        # Fallback logic based on category\n",
    "        if state['category'] == \"Technical\":\n",
    "            agent = \"Tech Support\"\n",
    "        elif state['category'] == \"Billing\":\n",
    "            agent = \"Billing Specialist\"\n",
    "        else:\n",
    "            agent = \"General Support\"\n",
    "\n",
    "    step_msg = f\"Assigned to {agent} by AI matching based on {state['category']}/{state['priority']}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": agent,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Assigned\"\n",
    "    }\n",
    "\n",
    "# Simple resolution generator for the demo\n",
    "def generate_resolution(state: TicketState) -> dict:\n",
    "    \"\"\"Use LLM to generate a resolution for the ticket\"\"\"\n",
    "\n",
    "    resolution_prompt = f\"\"\"\n",
    "    Generate a brief, professional resolution message for this support ticket:\n",
    "\n",
    "    Ticket: {state['title']}\n",
    "    Category: {state['category']}\n",
    "    Priority: {state['priority']}\n",
    "    Assigned Agent: {state['assigned_agent']}\n",
    "    Customer Tier: {state['customer_tier']}\n",
    "\n",
    "    Create a resolution message (max 30 words) that confirms the issue was addressed.\n",
    "    Be specific to the issue type and professional.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a support resolution specialist.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    return {\n",
    "        \"resolution\": response.content.strip(),\n",
    "        \"processing_steps\": state[\"processing_steps\"] + [\"Resolution generated by AI\"],\n",
    "        \"status\": \"Resolved\"\n",
    "    }\n",
    "\n",
    "# Demonstrate AI-powered state evolution\n",
    "def demonstrate_ai_workflow():\n",
    "    \"\"\"Process a ticket through AI-powered workflow stages\"\"\"\n",
    "\n",
    "    print(\"ü§ñ AI-Powered Ticket Processing Workflow\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Initial ticket state\n",
    "    state = TicketState(\n",
    "        ticket_id=\"TKT-12345\",\n",
    "        title=\"Cannot login to mobile app - urgent!\",\n",
    "        description=\"Login button not responding on iOS app, web version works fine. This is blocking my work.\",\n",
    "        customer_tier=\"Premium\",\n",
    "        category=\"\",\n",
    "        priority=\"\",\n",
    "        assigned_agent=\"\",\n",
    "        processing_steps=[],\n",
    "        status=\"New\",\n",
    "        resolution=\"\"\n",
    "    )\n",
    "\n",
    "    print(\"\\nüì• Initial Ticket:\")\n",
    "    print(f\"  ID: {state['ticket_id']}\")\n",
    "    print(f\"  Title: {state['title']}\")\n",
    "    print(f\"  Customer: {state['customer_tier']} tier\")\n",
    "    print(f\"  Status: {state['status']}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Starting AI Processing...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Step 1: AI Classification\n",
    "    print(\"\\nüîç Step 1: AI Classification\")\n",
    "    print(\"-\" * 30)\n",
    "    classification_result = classify_ticket(state)\n",
    "    state.update(classification_result)\n",
    "    print(f\"  ‚úÖ AI Decision: {state['category']}\")\n",
    "    print(f\"  üìù Reasoning: {state['processing_steps'][-1]}\")\n",
    "\n",
    "    # Step 2: AI Priority Assignment\n",
    "    print(\"\\n‚ö° Step 2: AI Priority Assignment\")\n",
    "    print(\"-\" * 30)\n",
    "    priority_result = route_priority(state)\n",
    "    state[\"processing_steps\"] = state[\"processing_steps\"] + priority_result[\"processing_steps\"]\n",
    "    state[\"priority\"] = priority_result[\"priority\"]\n",
    "    state[\"status\"] = priority_result[\"status\"]\n",
    "    print(f\"  ‚úÖ AI Decision: {state['priority']} priority\")\n",
    "    print(f\"  üìù Reasoning: {priority_result['processing_steps'][0]}\")\n",
    "\n",
    "    # Step 3: AI Agent Matching\n",
    "    print(\"\\nüë§ Step 3: AI Agent Matching\")\n",
    "    print(\"-\" * 30)\n",
    "    agent_result = match_agent(state)\n",
    "    state[\"processing_steps\"] = state[\"processing_steps\"] + agent_result[\"processing_steps\"]\n",
    "    state[\"assigned_agent\"] = agent_result[\"assigned_agent\"]\n",
    "    state[\"status\"] = agent_result[\"status\"]\n",
    "    print(f\"  ‚úÖ AI Decision: {state['assigned_agent']}\")\n",
    "    print(f\"  üìù Reasoning: {agent_result['processing_steps'][0]}\")\n",
    "\n",
    "    # Step 4: AI Resolution Generation\n",
    "    print(\"\\n‚úÖ Step 4: AI Resolution Generation\")\n",
    "    print(\"-\" * 30)\n",
    "    resolution_result = generate_resolution(state)\n",
    "    state.update(resolution_result)\n",
    "    print(f\"  ‚úÖ AI Resolution: {state['resolution']}\")\n",
    "    print(f\"  üìù Status: {state['status']}\")\n",
    "\n",
    "    # Final Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üìä Final State Summary\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"  Ticket ID: {state['ticket_id']}\")\n",
    "    print(f\"  Category: {state['category']}\")\n",
    "    print(f\"  Priority: {state['priority']}\")\n",
    "    print(f\"  Assigned To: {state['assigned_agent']}\")\n",
    "    print(f\"  Status: {state['status']}\")\n",
    "    print(f\"  Resolution: {state['resolution']}\")\n",
    "\n",
    "    print(\"\\nüîÑ Complete Processing Trail:\")\n",
    "    for i, step in enumerate(state['processing_steps'], 1):\n",
    "        print(f\"  {i}. {step}\")\n",
    "\n",
    "    return state\n",
    "\n",
    "# Run the AI-powered demonstration\n",
    "try:\n",
    "    final_state = demonstrate_ai_workflow()\n",
    "\n",
    "    print(\"\\n‚ú® Key Observations:\")\n",
    "    print(\"  ‚Ä¢ Each decision was made by AI analyzing the context\")\n",
    "    print(\"  ‚Ä¢ State evolved through the workflow with full history\")\n",
    "    print(\"  ‚Ä¢ Business logic was applied intelligently, not hard-coded\")\n",
    "    print(\"  ‚Ä¢ Complete audit trail maintained for compliance\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è Demo Error: {str(e)}\")\n",
    "    print(\"\\nThis might be due to:\")\n",
    "    print(\"  ‚Ä¢ Missing OpenAI API key in environment variables\")\n",
    "    print(\"  ‚Ä¢ API rate limits or network issues\")\n",
    "    print(\"  ‚Ä¢ Please ensure OPENAI_API_KEY is set in your .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0n8h7adq8wz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the demonstration workflow as a LangGraph\n",
    "def create_demo_workflow():\n",
    "    \"\"\"Build the AI demonstration workflow as a graph\"\"\"\n",
    "\n",
    "    # Initialize the StateGraph\n",
    "    workflow = StateGraph(TicketState)\n",
    "\n",
    "    # Add the 4 nodes from our demonstration\n",
    "    workflow.add_node(\"classify\", classify_ticket)\n",
    "    workflow.add_node(\"prioritize\", route_priority)\n",
    "    workflow.add_node(\"assign\", match_agent)\n",
    "    workflow.add_node(\"resolve\", generate_resolution)\n",
    "\n",
    "    # Add sequential edges connecting all nodes\n",
    "    workflow.add_edge(\"classify\", \"prioritize\")\n",
    "    workflow.add_edge(\"prioritize\", \"assign\")\n",
    "    workflow.add_edge(\"assign\", \"resolve\")\n",
    "    workflow.add_edge(\"resolve\", END)\n",
    "\n",
    "    # Set the entry point\n",
    "    workflow.set_entry_point(\"classify\")\n",
    "\n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Create and visualize the demo workflow\n",
    "demo_app = create_demo_workflow()\n",
    "\n",
    "print(\"üîç AI Workflow Graph - The Path We're About to Execute:\")\n",
    "print(\"=\" * 55)\n",
    "display(Image(demo_app.get_graph().draw_mermaid_png()))\n",
    "\n",
    "print(\"\\nüìù Graph Explanation:\")\n",
    "print(\"  ‚Ä¢ Each box represents an AI-powered decision node\")\n",
    "print(\"  ‚Ä¢ Arrows show the flow of state through the workflow\")\n",
    "print(\"  ‚Ä¢ The state accumulates information at each step\")\n",
    "print(\"  ‚Ä¢ Every node uses LLM intelligence for decisions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1serg5e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's execute the workflow using the graph we just visualized\n",
    "def execute_graph_workflow():\n",
    "    \"\"\"Execute the workflow through the LangGraph we created\"\"\"\n",
    "\n",
    "    print(\"üöÄ Executing the Workflow Through LangGraph\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Prepare a test ticket\n",
    "    test_ticket = {\n",
    "        \"ticket_id\": \"TKT-GRAPH-001\",\n",
    "        \"title\": \"Mobile app crashes on startup\",\n",
    "        \"description\": \"The iOS app crashes immediately after opening. This is affecting my productivity.\",\n",
    "        \"customer_tier\": \"Premium\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"\\nüì• Input Ticket:\")\n",
    "    print(f\"  ID: {test_ticket['ticket_id']}\")\n",
    "    print(f\"  Title: {test_ticket['title']}\")\n",
    "    print(f\"  Customer: {test_ticket['customer_tier']} tier\")\n",
    "    print(f\"  Status: {test_ticket['status']}\")\n",
    "\n",
    "    print(\"\\nüîÑ Processing through LangGraph...\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    try:\n",
    "        # Execute the workflow using the graph\n",
    "        result = demo_app.invoke(test_ticket)\n",
    "\n",
    "        print(\"\\n‚úÖ Graph Execution Complete!\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        print(\"\\nüìä Final State After Graph Processing:\")\n",
    "        print(f\"  Ticket ID: {result['ticket_id']}\")\n",
    "        print(f\"  Category: {result['category']}\")\n",
    "        print(f\"  Priority: {result['priority']}\")\n",
    "        print(f\"  Assigned Agent: {result['assigned_agent']}\")\n",
    "        print(f\"  Status: {result['status']}\")\n",
    "        print(f\"  Resolution: {result['resolution']}\")\n",
    "\n",
    "        print(\"\\nüîÑ Execution Path Through Graph:\")\n",
    "        for i, step in enumerate(result['processing_steps'], 1):\n",
    "            print(f\"  {i}. {step}\")\n",
    "\n",
    "        print(\"\\nüí° Key Insights:\")\n",
    "        print(\"  ‚Ä¢ The graph orchestrated the entire workflow\")\n",
    "        print(\"  ‚Ä¢ Each node processed the state sequentially\")\n",
    "        print(\"  ‚Ä¢ State accumulated through the execution path\")\n",
    "        print(\"  ‚Ä¢ LLM decisions were made at each node\")\n",
    "\n",
    "        return result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ö†Ô∏è Execution Error: {str(e)}\")\n",
    "        print(\"\\nThis might be due to:\")\n",
    "        print(\"  ‚Ä¢ Missing OpenAI API key\")\n",
    "        print(\"  ‚Ä¢ API rate limits\")\n",
    "        print(\"  ‚Ä¢ Network issues\")\n",
    "        return None\n",
    "\n",
    "# Execute the workflow through the graph\n",
    "print(\"Now let's run a ticket through the graph we just visualized:\\n\")\n",
    "graph_result = execute_graph_workflow()\n",
    "\n",
    "if graph_result:\n",
    "    print(\"\\nüéØ Graph-Based Workflow Success!\")\n",
    "    print(\"The LangGraph successfully processed the ticket through all nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6oskz557z5s",
   "metadata": {},
   "source": [
    "## üéØ AI-Powered Workflow Demonstration\n",
    "\n",
    "Now let's see how LangGraph manages state evolution through the **actual AI-powered workflow** we just visualized. We'll process a support ticket through four key stages, with real LLM decision-making at each step.\n",
    "\n",
    "### What We're Building:\n",
    "A **Customer Support Ticket Routing System** that uses AI to:\n",
    "\n",
    "1. **üìÇ Classification**: AI analyzes the ticket content and categorizes it (Technical/Billing/General)\n",
    "2. **‚ö° Priority Assignment**: AI determines urgency based on category, customer tier, and issue severity\n",
    "3. **üë§ Agent Matching**: AI matches the ticket to the best available support agent based on skills\n",
    "4. **‚úÖ Resolution**: AI generates a professional resolution summary\n",
    "\n",
    "### Key Insight:\n",
    "Unlike traditional workflows with hard-coded rules, each decision is made by an LLM that considers context, applies business logic, and makes intelligent routing decisions. The state evolves as it flows through the workflow, building a complete audit trail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25q858yn22o",
   "metadata": {},
   "source": [
    "### üéâ State Evolution with AI Complete!\n",
    "\n",
    "We've just demonstrated how **state flows through an AI-powered workflow** with real LLM decision-making at each step. The state evolved from an empty ticket to a fully processed and resolved case, with complete tracking of every AI decision.\n",
    "\n",
    "**Next**: Let's dive deeper into how LangGraph's **Nodes and Edges** architecture makes these complex workflows possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nodes-edges-concept",
   "metadata": {},
   "source": [
    "## üîò Nodes & Edges\n",
    "\n",
    "**Nodes** and **Edges** are the building blocks of LangGraph workflows. Together, they create sophisticated processing pipelines.\n",
    "\n",
    "### Nodes: State Processing Functions\n",
    "- **Input**: Current state\n",
    "- **Processing**: Business logic, LLM calls, API requests\n",
    "- **Output**: Updated state (partial or complete)\n",
    "- **Features**: Error handling, logging, side effects\n",
    "\n",
    "### Edges: Flow Control Mechanisms\n",
    "- **Fixed Edges**: Always go to the same next node\n",
    "- **Conditional Edges**: Dynamic routing based on state\n",
    "- **END Edge**: Terminate workflow\n",
    "\n",
    "### Node-to-Node Communication:\n",
    "Unlike function chaining, nodes communicate through **shared state**. Each node sees the complete workflow history and can make informed decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-concept",
   "metadata": {},
   "source": [
    "## üîÄ Conditional Edges & Dynamic Workflows\n",
    "\n",
    "**Conditional Edges** are where LangGraph truly shines. Unlike fixed linear flows, conditional edges enable **dynamic routing** based on state values, creating sophisticated branching workflows.\n",
    "\n",
    "### Conditional Edge Capabilities:\n",
    "- **Dynamic Routing**: Different paths based on state conditions\n",
    "- **Multi-Path Logic**: Handle success/error/retry scenarios\n",
    "- **State-Based Decisions**: Route based on any state field\n",
    "- **Complex Conditions**: Multiple criteria and nested logic\n",
    "- **Cycles & Loops**: Retry mechanisms and iterative processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-nodes",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced state for conditional workflows\n",
    "class EnhancedTicketState(TypedDict):\n",
    "    \"\"\"Enhanced state with conditional routing fields\"\"\"\n",
    "    # Basic ticket info\n",
    "    ticket_id: str\n",
    "    title: str\n",
    "    description: str\n",
    "    customer_tier: str\n",
    "\n",
    "    # Processing state\n",
    "    category: str\n",
    "    priority: str\n",
    "    assigned_agent: str\n",
    "\n",
    "    # Conditional routing fields\n",
    "    needs_escalation: bool\n",
    "\n",
    "    # Workflow tracking\n",
    "    processing_steps: Annotated[List[str], lambda x, y: x + y]\n",
    "    status: str\n",
    "    resolution: str\n",
    "\n",
    "# Initialize LLMs for different agent levels\n",
    "llm_junior = ChatOpenAI(model=\"gpt-4o-mini\")  # Normal agent\n",
    "llm_senior = ChatOpenAI(model=\"gpt-4\")  # Senior agent (more capable)\n",
    "\n",
    "print(\"ü§ñ LLM Agents Initialized:\")\n",
    "print(\"  ‚Ä¢ Junior Agent: GPT-4o-mini (efficient for standard issues)\")\n",
    "print(\"  ‚Ä¢ Senior Agent: GPT-4 (advanced reasoning for complex issues)\")\n",
    "\n",
    "# Node for escalation check\n",
    "def escalation_check(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Use LLM to determine if ticket needs escalation to senior agent\"\"\"\n",
    "\n",
    "    print(f\"Checking if ticket needs escalation...\")\n",
    "\n",
    "    # Use LLM for intelligent escalation decision\n",
    "    escalation_prompt = f\"\"\"\n",
    "    Analyze if this ticket needs escalation to a senior agent:\n",
    "\n",
    "    Ticket Details:\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "    - Category: {state['category']}\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "\n",
    "    Escalation Criteria:\n",
    "    - High priority tickets should be escalated\n",
    "    - Premium customers with technical issues should be escalated\n",
    "    - Any mention of \"critical\", \"urgent\", \"outage\", \"production\", or \"down\" should be escalated\n",
    "    - Complex technical problems requiring deep expertise should be escalated\n",
    "\n",
    "    Respond with ONLY \"YES\" or \"NO\" for escalation.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are an escalation specialist who determines if tickets need senior attention.\"),\n",
    "        HumanMessage(content=escalation_prompt)\n",
    "    ])\n",
    "\n",
    "    needs_escalation = response.content.strip().upper() == \"YES\"\n",
    "\n",
    "    step_msg = f\"Escalation decision: {'Route to Senior Agent (GPT-4)' if needs_escalation else 'Route to Normal Agent (GPT-4o-mini)'}\"\n",
    "\n",
    "    return {\n",
    "        \"needs_escalation\": needs_escalation,\n",
    "        \"processing_steps\": [step_msg],\n",
    "        \"status\": \"Escalation Checked\"\n",
    "    }\n",
    "\n",
    "# Senior Agent resolution (using GPT-4)\n",
    "def senior_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Senior agent with GPT-4 handles complex escalated tickets\"\"\"\n",
    "\n",
    "    print(f\"üî¥ SENIOR AGENT (GPT-4) handling escalated ticket\")\n",
    "\n",
    "    # Use more capable GPT-4 for complex problem solving\n",
    "    resolution_prompt = f\"\"\"\n",
    "    You are a SENIOR support agent with advanced expertise. This ticket has been escalated to you.\n",
    "\n",
    "    Ticket Details:\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "    - Customer Tier: {state['customer_tier']}\n",
    "\n",
    "    As a senior agent with GPT-4 capabilities, provide:\n",
    "    1. Deep technical analysis of the issue\n",
    "    2. Comprehensive solution with detailed steps\n",
    "    3. Preventive measures to avoid future occurrences\n",
    "\n",
    "    Create a detailed resolution (50-75 words) that demonstrates senior-level expertise.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_senior.invoke([\n",
    "        SystemMessage(content=\"You are a senior technical expert with deep knowledge and problem-solving capabilities.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    resolution_msg = f\"[SENIOR AGENT - GPT-4] {response.content.strip()}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Senior Agent (GPT-4)\",\n",
    "        \"resolution\": resolution_msg,\n",
    "        \"processing_steps\": [\"Escalated ticket resolved by Senior Agent using GPT-4's advanced capabilities\"],\n",
    "        \"status\": \"Resolved by Senior Agent\"\n",
    "    }\n",
    "\n",
    "# Normal Agent resolution (using GPT-4o-mini)\n",
    "def normal_agent_resolution(state: EnhancedTicketState) -> dict:\n",
    "    \"\"\"Normal agent with GPT-4o-mini handles standard tickets\"\"\"\n",
    "\n",
    "    print(f\"üü¢ NORMAL AGENT (GPT-4o-mini) handling standard ticket\")\n",
    "\n",
    "    # Use efficient GPT-4o-mini for standard issues\n",
    "    resolution_prompt = f\"\"\"\n",
    "    You are a standard support agent handling routine tickets.\n",
    "\n",
    "    Ticket Details:\n",
    "    - Title: {state['title']}\n",
    "    - Description: {state['description']}\n",
    "    - Category: {state['category']}\n",
    "    - Priority: {state['priority']}\n",
    "\n",
    "    Provide a helpful resolution (25-30 words) for this standard issue.\n",
    "    Be concise and practical.\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm_junior.invoke([\n",
    "        SystemMessage(content=\"You are a helpful support agent handling routine customer issues.\"),\n",
    "        HumanMessage(content=resolution_prompt)\n",
    "    ])\n",
    "\n",
    "    resolution_msg = f\"[NORMAL AGENT - GPT-4o-mini] {response.content.strip()}\"\n",
    "\n",
    "    return {\n",
    "        \"assigned_agent\": \"Normal Agent (GPT-4o-mini)\",\n",
    "        \"resolution\": resolution_msg,\n",
    "        \"processing_steps\": [\"Standard ticket resolved by Normal Agent using GPT-4o-mini\"],\n",
    "        \"status\": \"Resolved by Normal Agent\"\n",
    "    }\n",
    "\n",
    "print(\"\\nüìä Conditional Workflow Architecture:\")\n",
    "print(\"  üîÄ Escalation Check: AI determines ticket complexity\")\n",
    "print(\"  üî¥ Escalated Path ‚Üí Senior Agent (GPT-4)\")\n",
    "print(\"     ‚Ä¢ Handles complex technical issues\")\n",
    "print(\"     ‚Ä¢ Provides deep analysis and comprehensive solutions\")\n",
    "print(\"     ‚Ä¢ No approval needed - senior agents have authority\")\n",
    "print(\"  üü¢ Normal Path ‚Üí Normal Agent (GPT-4o-mini)\")\n",
    "print(\"     ‚Ä¢ Handles routine customer issues\")\n",
    "print(\"     ‚Ä¢ Provides quick, practical solutions\")\n",
    "print(\"     ‚Ä¢ Efficient processing for standard tickets\")\n",
    "\n",
    "print(\"\\nüí° Key Difference:\")\n",
    "print(\"  ‚Ä¢ Escalated tickets get MORE CAPABLE AI (GPT-4)\")\n",
    "print(\"  ‚Ä¢ Normal tickets get EFFICIENT AI (GPT-4o-mini)\")\n",
    "print(\"  ‚Ä¢ Different paths lead to DIFFERENT OUTCOMES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-routing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conditional routing functions\n",
    "def route_after_escalation_check(state: EnhancedTicketState) -> Literal[\"senior_agent\", \"normal_agent\"]:\n",
    "    \"\"\"Route based on escalation needs\"\"\"\n",
    "    if state['needs_escalation']:\n",
    "        return \"senior_agent\"  # Goes to GPT-4 powered senior agent\n",
    "    else:\n",
    "        return \"normal_agent\"  # Goes to GPT-4o-mini powered normal agent\n",
    "\n",
    "print(\"üîÄ Routing Logic Defined:\")\n",
    "print(\"  ‚Ä¢ needs_escalation = True ‚Üí 'senior_agent' (GPT-4)\")\n",
    "print(\"  ‚Ä¢ needs_escalation = False ‚Üí 'normal_agent' (GPT-4o-mini)\")\n",
    "print(\"\\n‚ú® This creates truly different paths with different AI capabilities!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "build-conditional-graph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the conditional workflow graph with different agent paths\n",
    "def create_conditional_workflow():\n",
    "    \"\"\"Create a workflow with truly different conditional paths\"\"\"\n",
    "\n",
    "    # Initialize the StateGraph with enhanced state\n",
    "    workflow = StateGraph(EnhancedTicketState)\n",
    "\n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"classifier\", classify_ticket)\n",
    "    workflow.add_node(\"priority_router\", route_priority)\n",
    "    workflow.add_node(\"escalation_check\", escalation_check)\n",
    "    workflow.add_node(\"senior_agent\", senior_agent_resolution)  # GPT-4 powered\n",
    "    workflow.add_node(\"normal_agent\", normal_agent_resolution)  # GPT-4o-mini powered\n",
    "\n",
    "    # Add fixed edges for initial processing\n",
    "    workflow.add_edge(\"classifier\", \"priority_router\")\n",
    "    workflow.add_edge(\"priority_router\", \"escalation_check\")\n",
    "\n",
    "    # Add conditional edge after escalation check - KEY DIFFERENCE!\n",
    "    workflow.add_conditional_edges(\n",
    "        \"escalation_check\",\n",
    "        route_after_escalation_check,\n",
    "        {\n",
    "            \"senior_agent\": \"senior_agent\",  # Escalated ‚Üí GPT-4\n",
    "            \"normal_agent\": \"normal_agent\"   # Normal ‚Üí GPT-4o-mini\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Both agents complete the resolution - no more approval gates\n",
    "    workflow.add_edge(\"senior_agent\", END)\n",
    "    workflow.add_edge(\"normal_agent\", END)\n",
    "\n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"classifier\")\n",
    "\n",
    "    # Compile the graph\n",
    "    app = workflow.compile()\n",
    "\n",
    "    return app\n",
    "\n",
    "# Create the conditional workflow\n",
    "conditional_app = create_conditional_workflow()\n",
    "\n",
    "print(\"üéØ Conditional Workflow Created!\")\n",
    "print(\"\\nüìä Workflow Paths:\")\n",
    "print(\"  Path 1 (Escalated): classify ‚Üí prioritize ‚Üí escalation_check ‚Üí senior_agent (GPT-4) ‚Üí END\")\n",
    "print(\"  Path 2 (Normal): classify ‚Üí prioritize ‚Üí escalation_check ‚Üí normal_agent (GPT-4o-mini) ‚Üí END\")\n",
    "print(\"\\nüí° Key Innovation:\")\n",
    "print(\"  ‚Ä¢ Different paths use DIFFERENT AI MODELS\")\n",
    "print(\"  ‚Ä¢ Senior path gets more capable but expensive GPT-4\")\n",
    "print(\"  ‚Ä¢ Normal path gets efficient GPT-4o-mini\")\n",
    "print(\"  ‚Ä¢ No redundant approval gates - agents resolve directly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yqdrnitaci",
   "metadata": {},
   "source": [
    "### Workflow Architecture:\n",
    "üì• **Entry:** classifier  \n",
    "üîç **Initial Processing:** classifier ‚Üí priority_router ‚Üí escalation_check  \n",
    "üîÄ **Conditional Branch:** escalation_check ‚Üí [senior_agent | normal_agent]  \n",
    "üèÅ **Exit:** [senior_agent | normal_agent] ‚Üí END  \n",
    "\n",
    "### Two Distinct Paths:\n",
    "üî¥ **Senior Path (Escalated):**\n",
    "- Uses GPT-4 (more capable, expensive)\n",
    "- Handles complex technical issues\n",
    "- Provides comprehensive solutions\n",
    "- Direct resolution without approval\n",
    "\n",
    "üü¢ **Normal Path (Standard):**\n",
    "- Uses GPT-4o-mini (efficient, cost-effective)\n",
    "- Handles routine inquiries\n",
    "- Provides quick solutions\n",
    "- Direct resolution without bureaucracy\n",
    "\n",
    "### Key Innovation:\n",
    "‚úÖ **Different AI Models for Different Needs**\n",
    "‚úÖ **Cost Optimization Through Intelligent Routing**\n",
    "‚úÖ **No Redundant Approval Gates**\n",
    "‚úÖ **Clear Path Differentiation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oewwhu876vg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the conditional workflow graph with all edges\n",
    "print(\"üìä Conditional Workflow Graph Visualization:\")\n",
    "print(\"Notice the branching paths showing conditional routing!\\n\")\n",
    "display(Image(conditional_app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-conditional-scenarios",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different conditional scenarios showing different AI agents\n",
    "def test_conditional_scenarios():\n",
    "    \"\"\"Test scenarios to show GPT-4 vs GPT-4o-mini routing\"\"\"\n",
    "\n",
    "    print(\"üß™ Testing Conditional Workflow - Different AI Agents\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Scenario 1: Critical issue - should go to Senior Agent (GPT-4)\n",
    "    print(\"\\nüìõ Scenario 1: Critical Production Outage\")\n",
    "    print(\"-\" * 50)\n",
    "    scenario1 = {\n",
    "        \"ticket_id\": \"TKT-CRITICAL\",\n",
    "        \"title\": \"URGENT: Production database down - all users affected\",\n",
    "        \"description\": \"Critical system failure, complete outage, revenue impact\",\n",
    "        \"customer_tier\": \"Premium\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"needs_escalation\": False,\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"Expected: ‚Üí Senior Agent (GPT-4) for complex resolution\")\n",
    "\n",
    "    # Scenario 2: Simple question - should go to Normal Agent (GPT-4o-mini)\n",
    "    print(\"\\nüí¨ Scenario 2: Basic Account Question\")\n",
    "    print(\"-\" * 50)\n",
    "    scenario2 = {\n",
    "        \"ticket_id\": \"TKT-SIMPLE\",\n",
    "        \"title\": \"How do I change my email address?\",\n",
    "        \"description\": \"I want to update my account email\",\n",
    "        \"customer_tier\": \"Standard\",\n",
    "        \"category\": \"\",\n",
    "        \"priority\": \"\",\n",
    "        \"assigned_agent\": \"\",\n",
    "        \"needs_escalation\": False,\n",
    "        \"processing_steps\": [],\n",
    "        \"status\": \"New\",\n",
    "        \"resolution\": \"\"\n",
    "    }\n",
    "\n",
    "    print(\"Expected: ‚Üí Normal Agent (GPT-4o-mini) for quick resolution\")\n",
    "\n",
    "    # Test both scenarios\n",
    "    scenarios = [(\"Critical\", scenario1), (\"Simple\", scenario2)]\n",
    "\n",
    "    for scenario_name, initial_state in scenarios:\n",
    "        print(f\"\\nüîÑ Executing {scenario_name} Scenario...\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        try:\n",
    "            result = conditional_app.invoke(initial_state)\n",
    "\n",
    "            print(f\"\\n‚úÖ {scenario_name} Scenario Completed!\")\n",
    "            print(f\"  üìä Results:\")\n",
    "            print(f\"  ‚Ä¢ Category: {result['category']}\")\n",
    "            print(f\"  ‚Ä¢ Priority: {result['priority']}\")\n",
    "            print(f\"  ‚Ä¢ Escalated: {result['needs_escalation']}\")\n",
    "            print(f\"  ‚Ä¢ Assigned to: {result['assigned_agent']}\")\n",
    "\n",
    "            print(f\"\\n  üìù Resolution:\")\n",
    "            print(f\"  {result['resolution'][:200]}...\")\n",
    "\n",
    "            print(f\"\\n  üîÑ Execution Path:\")\n",
    "            for i, step in enumerate(result['processing_steps'], 1):\n",
    "                print(f\"    {i}. {step}\")\n",
    "\n",
    "            # Highlight the key difference\n",
    "            if \"GPT-4)\" in result['assigned_agent']:\n",
    "                print(\"\\n  üî¥ Used SENIOR AGENT with GPT-4 (more capable)\")\n",
    "            else:\n",
    "                print(\"\\n  üü¢ Used NORMAL AGENT with GPT-4o-mini (efficient)\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Error: {str(e)}\")\n",
    "\n",
    "# Run the scenario tests\n",
    "test_conditional_scenarios()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üéØ Conditional Workflow Demonstration Complete!\")\n",
    "print(\"\\n‚ú® Key Achievements:\")\n",
    "print(\"  ‚Ä¢ Critical issues ‚Üí Senior Agent (GPT-4)\")\n",
    "print(\"  ‚Ä¢ Simple issues ‚Üí Normal Agent (GPT-4o-mini)\")\n",
    "print(\"  ‚Ä¢ Different paths = Different AI capabilities\")\n",
    "print(\"  ‚Ä¢ Cost optimization through intelligent routing\")\n",
    "print(\"  ‚Ä¢ No redundant approval gates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## üìã Summary\n",
    "\n",
    "This notebook demonstrated **LangGraph's sophisticated approach** to building stateful, graph-based AI workflows with LLM intelligence at every decision point.\n",
    "\n",
    "### üìä **State Management**\n",
    "‚úÖ **TypedDict schemas** - Type-safe, structured state definitions  \n",
    "‚úÖ **State evolution** - Data that flows and grows through workflow  \n",
    "‚úÖ **State reducers** - Intelligent handling of concurrent updates  \n",
    "‚úÖ **State inspection** - Debug and monitor workflow progress  \n",
    "\n",
    "### üîò **Nodes & Edges**\n",
    "‚úÖ **AI-powered nodes** - LLM intelligence in every processing function  \n",
    "‚úÖ **Fixed edges** - Deterministic workflow transitions  \n",
    "‚úÖ **Node communication** - Shared state instead of function chaining  \n",
    "‚úÖ **Error handling** - Graceful failure management with validation  \n",
    "\n",
    "### üîÄ **Conditional Logic**\n",
    "‚úÖ **Dynamic routing** - State-based decision making  \n",
    "‚úÖ **Multi-path workflows** - Handle complex scenarios with AI  \n",
    "‚úÖ **Business rules** - LLM-enforced conditional logic  \n",
    "‚úÖ **Intelligent decisions** - AI-driven path selection  \n",
    "\n",
    "## üìö **Resources**\n",
    "\n",
    "- **Documentation**: [LangGraph Docs](https://langgraph.readthedocs.io/)\n",
    "- **GitHub**: [langchain-ai/langgraph](https://github.com/langchain-ai/langgraph)\n",
    "- **Tutorials**: LangChain Academy LangGraph course\n",
    "- **Community**: LangChain Discord #langgraph channel\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

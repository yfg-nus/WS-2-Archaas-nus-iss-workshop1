{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95161edb-938c-47a2-9b7a-834323213506",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "panel-layout": {
     "height": 77.734375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# ARCHAAS Workshop 1: Prompt Engineering\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "**Prompt engineering** is the practice of designing and optimizing inputs (prompts) to guide AI models \n",
    "in generating more accurate, useful, and relevant responses. It plays a crucial role in improving \n",
    "interactions with large language models (LLMs) such as OpenAI's GPT-4. For this notebook, we require you to have both Claude or OpenAI API access. \n",
    "\n",
    "## Basic Principles of Prompt Engineering\n",
    "\n",
    "1. **Clarity and Specificity**  \n",
    "   - Clearly define the task and avoid vague or ambiguous instructions.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     Bad: \"Tell me about history.\"\n",
    "     Good: \"Provide a summary of the Renaissance period and its impact on European art.\"\n",
    "     ```\n",
    "\n",
    "2. **Role Assignment**  \n",
    "   - Assign a persona or role to the AI model to get responses in a specific tone or expertise.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"You are an AI financial advisor. Explain the benefits of index funds to a beginner investor.\"\n",
    "     ```\n",
    "\n",
    "3. **Context and Constraints**  \n",
    "   - Provide relevant background information and define constraints such as format, length, or tone.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Write a 100-word product description for a new AI-powered smartphone.\"\n",
    "     ```\n",
    "\n",
    "4. **Step-by-Step Breakdown**  \n",
    "   - Ask the model to explain its reasoning in steps for complex tasks.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Explain the process of machine learning model training in a step-by-step manner.\"\n",
    "     ```\n",
    "\n",
    "5. **Examples and Formatting Guidance**  \n",
    "   - Show examples to guide the model on expected output formats.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"Translate the following English sentences to French:\\n1. Hello, how are you?\\n2. The weather is nice today.\"\n",
    "     ```\n",
    "\n",
    "6. **Iterative Refinement**  \n",
    "   - Experiment with different prompts and refine them based on the model's responses.  \n",
    "   - Example:\n",
    "     ```plaintext\n",
    "     \"List 5 ways AI is used in healthcare. If possible, provide real-world examples.\"\n",
    "     ```\n",
    "\n",
    "By following these principles, developers can craft effective prompts that enhance the performance of AI models.\n",
    "\"\"\"\n",
    "\n",
    "Python code can follow below if needed\n",
    "\n",
    "For this prompt engineering, we follow [Anthropic API fundamentals](https://github.com/anthropics/courses/tree/master/anthropic_api_fundamentals) and have a similar track with OpenAI API.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943a6b3f-9798-47b2-b1cc-08320654d6c4",
   "metadata": {
    "panel-layout": {
     "height": 112.015625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Step 1: Load Environment Variables and API Keys\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "In this step, we will ensure that the notebook can load the global `.env` file and that the required API keys are in place.\n",
    "\n",
    "## Objectives:\n",
    "1. Load environment variables using the `dotenv` package.\n",
    "2. Verify that API keys are set up correctly.\n",
    "3. Provide options to check API keys with and without displaying them.\n",
    "\n",
    "## Instructions:\n",
    "1. Ensure you have the `python-dotenv` package installed:\n",
    "   ```bash\n",
    "   pip install python-dotenv\n",
    "\n",
    "2. Create a .env file in the root directory (if not already created) and add your API keys:\n",
    "\n",
    "OPENAI_API_KEY=your_openai_api_key\n",
    "ANTHROPIC_API_KEY=your_anthropic_api_key\n",
    "\n",
    "3. Run the following Python code to load and verify the API keys.</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932f451c-0860-44d0-9c56-9df17823357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Python Code:\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch API keys\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# Check if API keys are loaded\n",
    "if openai_api_key and anthropic_api_key:\n",
    "    print(\"‚úÖ API keys are successfully loaded.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Warning: One or more API keys are missing.\")\n",
    "\n",
    "# Optionally, display API keys (for debugging purposes only)\n",
    "display_keys = False  # Change to True if you want to see the keys\n",
    "\n",
    "if display_keys:\n",
    "    print(f\"OpenAI API Key: {openai_api_key}\")\n",
    "    print(f\"Anthropic API Key: {anthropic_api_key}\")\n",
    "else:\n",
    "    print(\"üîí API keys are loaded but hidden for security.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aeaa12-7edf-4f8a-b761-16420d6954ad",
   "metadata": {
    "panel-layout": {
     "height": 105.671875,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "## Messages format and Understanding the concept of **role**, **user** and **content**\n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "\n",
    "### Message Format     \n",
    "As we saw in the previous lesson, we can use `client.messages.create()` (Claude) and `client.chat.completions.create` (Open AI) to send a message to Claude & OpenAI and get their respective responses.\n",
    "\n",
    "The messages format allows us to structure our API calls to Claude or GPT in the form of a conversation, allowing for **context preservation**: The messages format allows for maintaining an entire conversation history, including both user and assistant messages. This ensures that Claude or GPT has access to the full context of the conversation when generating responses, leading to more coherent and relevant outputs.  \n",
    "\n",
    "**Note: many use-cases don't require a conversation history, and there's nothing wrong with providing a list of messages that only contains a single message!** \n",
    "\n",
    "In addition to `content`, the `Message` object contains some other pieces of information:\n",
    "\n",
    "* `id` - a unique object identifier\n",
    "* `type` - The object type, which will always be \"message\"\n",
    "* `role` - The conversational role of the generated message. This will always be \"assistant\".\n",
    "* `model` - The model that handled the request and generated the response\n",
    "* `stop_reason` - The reason the model stopped generating.  We'll learn more about this later.\n",
    "* `stop_sequence` - We'll learn more about this shortly.\n",
    "* `usage` - information on billing and rate-limit usage. Contains information on:\n",
    "    * `input_tokens` - The number of input tokens that were used.\n",
    "    * `output_tokens` - The number of output tokens that were used.\n",
    "\n",
    "It's important to know that we have access to these pieces of information, but if you only remember one thing, make it this: `content` contains the actual model-generated content\n",
    "\n",
    "### How does **role**, **user** and **content** work? \n",
    "Let's take a closer look at this bit (whether it's Anthropic or OpenAI): \n",
    "```py\n",
    "messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavors are used in Dr. Pepper?\"}\n",
    "    ]\n",
    "```\n",
    "\n",
    "The messages parameter is a crucial part of interacting with the Claude and OpenAI API. It allows you to provide the conversation history and context for **Claude** or **OpenAI** to generate a relevant response. \n",
    "\n",
    "The messages parameter expects a list of message dictionaries, where each dictionary represents a single message in the conversation.\n",
    "\n",
    "Each message dictionary should have the following keys:\n",
    "\n",
    "* `role`: A string indicating the role of the message sender. It can be either \"user\" (for messages sent by the user) or \"assistant\" (for messages sent by Claude or OpenAI).\n",
    "* `content`: A string or list of content dictionaries representing the actual content of the message. If a string is provided, it will be treated as a single text content block. If a list of content dictionaries is provided, each dictionary should have a \"type\" (e.g., \"text\" or \"image\") and the corresponding content.  For now, we'll leave `content` as a single string.\n",
    "\n",
    "Here's an example of a messages list with a single user message:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude/OpenAI! How are you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "And here's an example with multiple messages representing a conversation:\n",
    "\n",
    "```py\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Hello Claude/OpenAI! How are you today?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! I'm doing well, thank you. How can I assist you today?\"},\n",
    "    {\"role\": \"user\", \"content\": \"Can you tell me a fun fact about ferrets?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Sure! Did you know that excited ferrets make a clucking vocalization known as 'dooking'?\"},\n",
    "]\n",
    "```\n",
    "\n",
    "Remember that messages always alternate between user and assistant messages.</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc04083-fbde-4063-aa42-64e6d9268c9b",
   "metadata": {
    "panel-layout": {
     "height": 0,
     "visible": true,
     "width": 100
    }
   },
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "\n",
    "\n",
    "# Call the Claude API\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97153c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr Pepper's exact recipe is a closely guarded secret, but the company describes the soda as having a unique blend of 23 flavors. These are speculated to include cherry, caramel, licorice, amaretto, vanilla, blackberry, apricot, blackberry, carrot, clove, and rum.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",  # Use \"gpt-4\" or \"gpt-3.5-turbo\" ‚Äî \"gpt-5\" is NOT a released model\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What flavours are used in Dr. Pepper?\"}\n",
    "    ],\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63678bde-2086-4809-9b40-e8964aa8f868",
   "metadata": {
    "panel-layout": {
     "height": 77.734375,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "# Model Parameters\n",
    "\n",
    "<details><summary>Click to expand/collapse</summary>\n",
    "\n",
    "### Lesson Goals\n",
    "* Understand the role of the `max_tokens` parameter.\n",
    "* Use the `temperature` parameter to control model responses.\n",
    "* Explain the purpose of `stop_sequence`.\n",
    "\n",
    "## Required Parameters\n",
    "\n",
    "When making a request to a Large Language Model (LLM) such as **Claude** (Anthropic) or **GPT** (OpenAI), there are three required parameters:\n",
    "\n",
    "* `model`\n",
    "* `max_tokens`\n",
    "* `messages`\n",
    "\n",
    "So far, we have been using the `max_tokens` parameter in every single request, but we have not stopped to discuss what it is.\n",
    "\n",
    "---\n",
    "\n",
    "### Using `max_tokens` in Claude\n",
    "\n",
    "Here is an example request to Claude:\n",
    "\n",
    "```python\n",
    "our_first_message = client.messages.create(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### Here is an equivalent request to OpenAI: \n",
    "\n",
    "```python\n",
    "our_first_message = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    max_tokens=500,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! Please write me a haiku about a pet chicken\"}\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "### So What is the Purpose of max_tokens?\n",
    "In short, **max_tokens** controls the maximum number of tokens that the model should generate in its response. \n",
    "\n",
    "Before we go any further, let us pause for a moment to discuss tokens.\n",
    "\n",
    "Most Large Language Models don't think in full words but instead process and generate responses using tokens, which are small building blocks of a text sequence.\n",
    "\n",
    "When we provide a prompt to an LLM, the model:\n",
    "\n",
    "Converts the input into tokens.\n",
    "Processes the tokens and generates the output one token at a time.\n",
    "\n",
    "### Token Differences Between Claude and OpenAI\n",
    "\n",
    "|**Model**                | **Approximate Token Size**         |\n",
    "|--------------------------|-----------------------------------|\n",
    "|**Claude (Anthropic)**   | 1 token ‚âà 3.5 English characters |\n",
    "|**OpenAI (GPT-4/GPT-3.5)** | 1 token ‚âà 4 English characters |\n",
    "\n",
    "*The exact token count may vary depending on the language and structure of the text.*\n",
    "\n",
    "### Why alter max tokens?\n",
    "Understanding tokens is crucial when working with Claude, particularly for the following reasons:\n",
    "\n",
    "* **API limits**: The number of tokens in your input text and the generated response count towards the API usage limits. Each API request has a maximum limit on the number of tokens it can process. Being aware of tokens helps you stay within the API limits and manage your usage efficiently.\n",
    "* **Performance**: The number of tokens Claude generates directly impacts the processing time and memory usage of the API. Longer input texts and higher max_tokens values require more computational resources. Understanding tokens helps you optimize your API requests for better performance.\n",
    "* **Response quality**: Setting an appropriate max_tokens value ensures that the generated response is of sufficient length and contains the necessary information. If the max_tokens value is too low, the response may be truncated or incomplete. Experimenting with different max_tokens values can help you find the optimal balance for your specific use case.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857db08c-bbfc-4e36-a7ac-a24c7b3583a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client (not OpenAI)\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "# Claude API with max_tokens and you can try max_tokens from 10 to 1000 and 500 is usually where the poem ends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a8a8d6-d41b-41cf-b2d8-72ea3fd14720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI client with API key from environment\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Check if API key is loaded correctly\n",
    "if not client.api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è OpenAI API key is missing. Please check your .env file.\")\n",
    "\n",
    "# Make API call with max_tokens set to 10 and try 100 or 500\n",
    "\n",
    "# Print truncated response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08690297-d85a-4d73-a71d-f66fa648d882",
   "metadata": {},
   "source": [
    "# Stop Sequences \n",
    "\n",
    "<details>\n",
    "<summary>Click to expand/collapse</summary>\n",
    "\n",
    "Another important parameter we haven't seen yet is `stop_sequence` which allows us to provide the model with a set of strings that, when encountered in the generated response, cause the generation to stop.  They are essentially a way of telling Claude or OpenAI, \"if you generate this sequence, stop generating anything else!\"\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df5609-8979-4332-ab6d-cf421c680644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of stop_sequence with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b6606c-e5ca-4b15-824a-57511c025c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of stop_sequence with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Make an API call with a stop sequence\n",
    "\n",
    "stop=[\"}\"] #stop closing for JSON\n",
    "\n",
    "# Print response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4bc6f2-c6f4-44e3-89e1-6ec319658dce",
   "metadata": {},
   "source": [
    "## Temperature\n",
    "\n",
    "The `temperature` parameter is used to control the \"randomness\" and \"creativity\" of the generated responses. It ranges from 0 to 1, with higher values resulting in more diverse and unpredictable responses with variations in phrasing.  Lower temperatures can result in more deterministic outputs that stick to the most probable phrasing and answers. **Temperature has a default value of 1**.\n",
    "\n",
    "When generating text, any LLM (Claude, GPT or DeepSeek) predicts the probability distribution of the next token (word or subword). The temperature parameter is used to manipulate this probability distribution before sampling the next token. If the temperature is low (close to 0.0), the probability distribution becomes more peaked, with high probabilities assigned to the most likely tokens. This makes the model more deterministic and focused on the most probable or \"safe\" choices. If the temperature is high (closer to 1.0), the probability distribution becomes more flattened, with the probabilities of less likely tokens increasing. This makes the model more random and exploratory, allowing for more diverse and creative outputs. \n",
    "\n",
    "See this diagram for a visual representation of the impact of temperature (Source: Anthropic):\n",
    "<img src=\"images/temperature.png\" alt=\"Chart description\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "Why would you change temperature?\n",
    "**Use temperature closer to 0.0 for analytical tasks, and closer to 1.0 for creative and generative tasks.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40f138c-af49-46ae-89a8-790efa095c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of Temperature with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "#Code for demonstrate_temperature()\n",
    "\n",
    "\n",
    "\n",
    "#Notice that with a temperature of 0, all three responses are the same.  \n",
    "#Note that even with a temperature of 0.0, the results will not be fully deterministic.  \n",
    "#However, there is a clear difference when compared to the results with a temperature of 1.  \n",
    "#Each response was a completely different alien planet name. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa033a3-cae7-49f5-96e6-f08aa35e73df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of Temperature with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "\n",
    "# Run the function\n",
    "def demonstrate_temperature():\n",
    "    temperatures = [0.0, 0.5, 1.0]\n",
    "    for temp in temperatures:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-5\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Suggest a name for an alien planet.\"\n",
    "                }\n",
    "            ],\n",
    "            temperature=temp,\n",
    "            max_completion_tokens=10\n",
    "        )\n",
    "        print(f\"Temperature: {temp}\")\n",
    "        print(f\"Response: {response.choices[0].message.content}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ddfd7-a6ca-46f4-89f7-dc1b06d7c0c9",
   "metadata": {},
   "source": [
    "## System Prompt\n",
    "\n",
    "The `system_prompt` is an optional parameter that you can include when sending messages to **Claude (Anthropic)** and **GPT-4 (OpenAI)**. It sets the stage for the conversation by providing high-level instructions, defining the AI's role, or giving background information that should inform its responses.\n",
    "\n",
    "### Key Points About the `system_prompt`:\n",
    "- It's **optional** but can be useful for setting the **tone** and **context** of the conversation.\n",
    "- It is applied at the **conversation level**, affecting all responses within that exchange.\n",
    "- It helps **steer the model‚Äôs behavior** without needing to include instructions in every user message.\n",
    "\n",
    "### How Each Model Uses the `system_prompt`:\n",
    "| **Model**   | **System Prompt Usage** |\n",
    "|------------|------------------------|\n",
    "| **Claude (Anthropic)** | Uses `system_prompt` to set high-level guidance for the conversation. |\n",
    "| **GPT-4 (OpenAI)** | Uses a system message as the first entry in `messages` (e.g., `{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}`). |\n",
    "\n",
    "---\n",
    "\n",
    "### **Best Practices for Using System Prompts**\n",
    "‚úÖ **Use it for high-level guidance** (e.g., defining tone, behavior, role).  \n",
    "‚úÖ **Avoid detailed instructions or long documents** in the system prompt.  \n",
    "‚úÖ **Provide detailed instructions** inside the **first User message** for better results.  \n",
    "‚úÖ **No need to repeat it** for every subsequent user turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886696a-916a-4f29-85e1-03e9cff55fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of System Prompts with Claude AI\n",
    "\n",
    "from anthropic import Anthropic\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the Anthropic client using environment variable\n",
    "client = Anthropic(api_key=os.getenv('ANTHROPIC_API_KEY'))\n",
    "\n",
    "#Write a system prompt that instructs the model to respond in Chinese.\n",
    "\n",
    "#print the response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db9d08-ffa7-4cfc-a755-0476cef9e4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of System Prompts with OpenAI API\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize the OpenAI client using environment variable\n",
    "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "# Create a chat completion with a system prompt\n",
    "\n",
    "# Print response\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "panel-cell-order": [
   "95161edb-938c-47a2-9b7a-834323213506",
   "943a6b3f-9798-47b2-b1cc-08320654d6c4",
   "37aeaa12-7edf-4f8a-b761-16420d6954ad",
   "bbc04083-fbde-4063-aa42-64e6d9268c9b",
   "87db25c7-3f65-4e79-8f92-9e812efc43f0",
   "63678bde-2086-4809-9b40-e8964aa8f868"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
